{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from keras_bert import Tokenizer\n",
    "\n",
    "VOCABULARY_PATH =  './vocab.txt'\n",
    "\n",
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(VOCABULARY_PATH, 'r', 'utf8') as file_reader:\n",
    "    for line in file_reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "\n",
    "def bert_vectorize_data(articles_text):\n",
    "    data_X = []\n",
    "\n",
    "    for text in articles_text:\n",
    "        ids, _ = tokenizer.encode(text, max_len=SEQUENCE_LENGTH)\n",
    "        data_X.append(ids)\n",
    "\n",
    "    data_X = np.array([data_X, np.zeros_like(data_X)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_articles = []\n",
    "staging_x = []\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "article_list_files = sorted(os.listdir('../../../data/article_list'))\n",
    "\n",
    "latest_article_list_filename = None if len(article_list_files) == 0 else article_list_files[-1]\n",
    "\n",
    "last_updated = latest_article_list_filename[:-4]\n",
    "\n",
    "article_list = pickle.load(open('../../../data/article_list/{}'.format(latest_article_list_filename), 'rb'))\n",
    "\n",
    "for article_id in article_list:\n",
    "    text = pickle.load(open('../../../data/articles/{}.pkl'.format(article_id), 'rb'))['text']\n",
    "    ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "    staging_x.append(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23772"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(staging_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(staging_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 898,\n",
       " 4212,\n",
       " 1922,\n",
       " 7382,\n",
       " 4496,\n",
       " 4495,\n",
       " 4229,\n",
       " 5543,\n",
       " 4638,\n",
       " 100,\n",
       " 3417,\n",
       " 5471,\n",
       " 6365,\n",
       " 100,\n",
       " 1353,\n",
       " 2746,\n",
       " 1333,\n",
       " 4415,\n",
       " 4777,\n",
       " 4634,\n",
       " 4638,\n",
       " 4634,\n",
       " 7442,\n",
       " 6172,\n",
       " 5390,\n",
       " 100,\n",
       " 782,\n",
       " 6863,\n",
       " 1922,\n",
       " 7382,\n",
       " 100,\n",
       " 8024,\n",
       " 679,\n",
       " 852,\n",
       " 679,\n",
       " 3298,\n",
       " 3300,\n",
       " 3417,\n",
       " 2450,\n",
       " 3160,\n",
       " 4496,\n",
       " 4495,\n",
       " 8024,\n",
       " 5445,\n",
       " 684,\n",
       " 7444,\n",
       " 6206,\n",
       " 1914,\n",
       " 2208,\n",
       " 7442,\n",
       " 5543,\n",
       " 2218,\n",
       " 5543,\n",
       " 4496,\n",
       " 4495,\n",
       " 1914,\n",
       " 2208,\n",
       " 7442,\n",
       " 5543,\n",
       " 511,\n",
       " 2792,\n",
       " 809,\n",
       " 6206,\n",
       " 2682,\n",
       " 1343,\n",
       " 3417,\n",
       " 7442,\n",
       " 5445,\n",
       " 679,\n",
       " 3174,\n",
       " 7442,\n",
       " 8024,\n",
       " 2218,\n",
       " 2553,\n",
       " 7519,\n",
       " 6656,\n",
       " 677,\n",
       " 686,\n",
       " 4518,\n",
       " 5543,\n",
       " 3975,\n",
       " 4638,\n",
       " 3173,\n",
       " 6638,\n",
       " 1248,\n",
       " 4500,\n",
       " 100,\n",
       " 6631,\n",
       " 2206,\n",
       " 4634,\n",
       " 7442,\n",
       " 100,\n",
       " 511,\n",
       " 3075,\n",
       " 3300,\n",
       " 7770,\n",
       " 4906,\n",
       " 2825,\n",
       " 4638,\n",
       " 1044,\n",
       " 6868,\n",
       " 1751,\n",
       " 2157,\n",
       " 4412,\n",
       " 791,\n",
       " 6963,\n",
       " 679,\n",
       " 3140,\n",
       " 6738,\n",
       " 6241,\n",
       " 2450,\n",
       " 3417,\n",
       " 8024,\n",
       " 5445,\n",
       " 684,\n",
       " 5160,\n",
       " 5160,\n",
       " 6206,\n",
       " 6518,\n",
       " 2549,\n",
       " 3176,\n",
       " 6818,\n",
       " 2407,\n",
       " 2399,\n",
       " 5173,\n",
       " 3632,\n",
       " 4125,\n",
       " 1213,\n",
       " 4634,\n",
       " 7442,\n",
       " 2449,\n",
       " 4638,\n",
       " 6243,\n",
       " 1205,\n",
       " 8024,\n",
       " 5445,\n",
       " 1378,\n",
       " 4124,\n",
       " 679,\n",
       " 1072,\n",
       " 6631,\n",
       " 2206,\n",
       " 4634,\n",
       " 7442,\n",
       " 722,\n",
       " 5543,\n",
       " 8024,\n",
       " 852,\n",
       " 1059,\n",
       " 686,\n",
       " 4518,\n",
       " 1320,\n",
       " 1546,\n",
       " 4360,\n",
       " 2769,\n",
       " 947,\n",
       " 1378,\n",
       " 4124,\n",
       " 8024,\n",
       " 679,\n",
       " 852,\n",
       " 6206,\n",
       " 5173,\n",
       " 3632,\n",
       " 3417,\n",
       " 7442,\n",
       " 8024,\n",
       " 5445,\n",
       " 684,\n",
       " 6917,\n",
       " 6206,\n",
       " 6624,\n",
       " 1726,\n",
       " 7531,\n",
       " 6662,\n",
       " 510,\n",
       " 1726,\n",
       " 3645,\n",
       " 5862,\n",
       " 2527,\n",
       " 8024,\n",
       " 5646,\n",
       " 2456,\n",
       " 3173,\n",
       " 4638,\n",
       " 4125,\n",
       " 1213,\n",
       " 4634,\n",
       " 7442,\n",
       " 2449,\n",
       " 8024,\n",
       " 2792,\n",
       " 4158,\n",
       " 862,\n",
       " 889,\n",
       " 8043,\n",
       " 4192,\n",
       " 7478,\n",
       " 3221,\n",
       " 3300,\n",
       " 3428,\n",
       " 2094,\n",
       " 2798,\n",
       " 3300,\n",
       " 3779,\n",
       " 3717,\n",
       " 8024,\n",
       " 3209,\n",
       " 3099,\n",
       " 5865,\n",
       " 2218,\n",
       " 3221,\n",
       " 3300,\n",
       " 782,\n",
       " 2100,\n",
       " 2552,\n",
       " 6206,\n",
       " 5964,\n",
       " 5865,\n",
       " 5646,\n",
       " 2456,\n",
       " 7431,\n",
       " 2279,\n",
       " 7591,\n",
       " 7442,\n",
       " 1350,\n",
       " 4125,\n",
       " 1213,\n",
       " 4634,\n",
       " 7442,\n",
       " 2449,\n",
       " 4634,\n",
       " 6512,\n",
       " 511,\n",
       " 4525,\n",
       " 4994,\n",
       " 8024,\n",
       " 2958,\n",
       " 3609,\n",
       " 1372,\n",
       " 3221,\n",
       " 671,\n",
       " 3229,\n",
       " 8024,\n",
       " 679,\n",
       " 6630,\n",
       " 3634,\n",
       " 3229,\n",
       " 6517,\n",
       " 3912,\n",
       " 2521,\n",
       " 862,\n",
       " 3229,\n",
       " 511,\n",
       " 8532,\n",
       " 131,\n",
       " 120,\n",
       " 120,\n",
       " 8357,\n",
       " 10455,\n",
       " 119,\n",
       " 8815,\n",
       " 120,\n",
       " 9499,\n",
       " 8171,\n",
       " 8160,\n",
       " 8253,\n",
       " 8206,\n",
       " 8810,\n",
       " 11472,\n",
       " 8159,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14908 files in raw_data\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "repos = ['raw_data']\n",
    "\n",
    "repo_files = []\n",
    "for repo in repos:\n",
    "    files = os.listdir(repo)\n",
    "    files = [file for file in files if 'json' in file]\n",
    "    \n",
    "    repo_files.append(files)\n",
    "\n",
    "    print('There are '+ str(len(files)) +' files in '+repo )\n",
    "\n",
    "# load tags and text information of the files\n",
    "# encoding: utf-8\n",
    "\n",
    "repo = repos[0]\n",
    "files = repo_files[0]\n",
    "\n",
    "#df = pd.DataFrame()\n",
    "#\n",
    "#for file in files:\n",
    "#    data = pd.read_json(os.path.join(repo, file))\n",
    "#    data['file_name'] = file\n",
    "#    df = df.append(data, ignore_index = True)\n",
    "\n",
    "tag_type = 17\n",
    "define_columns = ['id', 'text', 'tag_0', 'tag_1', 'tag_2', 'tag_3', 'tag_4', 'tag_5', 'tag_6',\n",
    "                 'tag_7', 'tag_8', 'tag_9', 'tag_10', 'tag_11', 'tag_12', 'tag_13', 'tag_14', 'tag_15', 'tag_16']\n",
    "\n",
    "data_list = []\n",
    "for file in files:\n",
    "    \n",
    "    with open(os.path.join(repo, file), 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    tags_list = [0]*tag_type\n",
    "    \n",
    "    for tag in data['tags']:\n",
    "        tags_list[tag] = 1\n",
    "    \n",
    "    # TBD: using file name or original id for modeling id\n",
    "    #data_list.append([data['id'], data['text']] + tags_list)\n",
    "    data_list.append([file, data['text']] + tags_list)\n",
    "\n",
    "df_data = pd.DataFrame(data_list, columns=define_columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.set_dictionary('./dict.txt.big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "\n",
    "def load_data(df_dataset):\n",
    "    \n",
    "    tokenized_text = []\n",
    "    \n",
    "    indices, labels = [], []\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    for row in df_dataset.iterrows():\n",
    "        text = row[1]['text']\n",
    "        tokenized_text.append(' '.join(jieba.cut(text, cut_all=True)))\n",
    "\n",
    "        label = list(row[1].iloc[2:])\n",
    "        label = label.index(max(label))\n",
    "        labels.append(label)\n",
    "\n",
    "    items = list(zip(tokenized_text, labels))\n",
    "    \n",
    "    np.random.shuffle(items)\n",
    "    test_items = items[int(0.8*len(items)):]\n",
    "    train_items = items[:int(0.8*len(items))]\n",
    "    \n",
    "    text_test, labels_test = zip(*test_items)\n",
    "    text_train, labels_train = zip(*train_items)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    return vectorizer.fit_transform(text_train), labels_train, vectorizer.transform(text_test), labels_test\n",
    "    \n",
    "train_x, train_y, test_x, test_y = load_data(df_data)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     indices_test = np.array(indices_test)\n",
    "#     indices_train = np.array(indices_train)\n",
    "\n",
    "#     \n",
    "  \n",
    "# train_x, train_y, test_x, test_y = load_data(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2982x89235 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 148116 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "meow = ['全由 觀光 觀光局 支付     相當 可疑', '自 中國 進口 口水 水壺 水壺蓋 壺蓋     太和 工房 被 爆 驗 出 殘渣 值 超標 57 倍']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(meow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.408248290463863\n",
      "  (0, 12)\t0.408248290463863\n",
      "  (0, 8)\t0.408248290463863\n",
      "  (0, 14)\t0.408248290463863\n",
      "  (0, 13)\t0.408248290463863\n",
      "  (0, 2)\t0.408248290463863\n",
      "  (1, 0)\t0.30151134457776363\n",
      "  (1, 15)\t0.30151134457776363\n",
      "  (1, 9)\t0.30151134457776363\n",
      "  (1, 7)\t0.30151134457776363\n",
      "  (1, 6)\t0.30151134457776363\n",
      "  (1, 5)\t0.30151134457776363\n",
      "  (1, 11)\t0.30151134457776363\n",
      "  (1, 10)\t0.30151134457776363\n",
      "  (1, 3)\t0.30151134457776363\n",
      "  (1, 16)\t0.30151134457776363\n",
      "  (1, 1)\t0.30151134457776363\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 3336, 7448, ...,  791, 2399,  102],\n",
       "       [ 101, 6857,  679, ...,    0,    0,    0],\n",
       "       [ 101, 3297, 6818, ..., 3596, 3389,  102],\n",
       "       ...,\n",
       "       [ 101, 7350, 2209, ...,    0,    0,    0],\n",
       "       [ 101, 1060, 4934, ...,    0,    0,    0],\n",
       "       [ 101,  521,  679, ..., 3189, 2405,  102]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11926,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14908x99547 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 816198 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6132819050813348\n",
      "0.5938967136150235\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=25, min_samples_leaf=30, max_features=0.1)\n",
    "clf.fit(train_x, train_y)\n",
    "print(clf.score(train_x, train_y))\n",
    "print(clf.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'全由 觀光 觀光局 支付     相當 可疑'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(jieba.cut('全由觀光局支付 相當可疑', cut_all=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 500 max_depth = 80 min_samples_leaf = 3 max_features = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8117558276035552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505700871898055\n",
      "new best! 0.6505700871898055\n"
     ]
    }
   ],
   "source": [
    "ESTIMATOR_N = [500]\n",
    "MAX_DEPTH = [80]\n",
    "MIN_SAMPLES_LEAF = [3]\n",
    "MAX_FEATURES = [0.1]\n",
    "\n",
    "best_score = 0\n",
    "best_params = (0,0,0,0)\n",
    "\n",
    "for E in ESTIMATOR_N:\n",
    "    for md in MAX_DEPTH:\n",
    "        for msl in MIN_SAMPLES_LEAF:\n",
    "            for mf in MAX_FEATURES:\n",
    "                print('n_estimators =', E, 'max_depth =', md, 'min_samples_leaf =', msl, 'max_features =', mf)\n",
    "                clf = RandomForestClassifier(n_estimators=E, max_depth=md, min_samples_leaf=msl, max_features=mf, verbose=True, n_jobs=4)\n",
    "                clf.fit(train_x, train_y)\n",
    "                print(clf.score(train_x, train_y))\n",
    "                test_score = clf.score(test_x, test_y)\n",
    "                print(test_score)\n",
    "                if test_score > best_score:\n",
    "                    best_score = test_score\n",
    "                    best_params = (E, md, msl, mf)\n",
    "                    print('new best!', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(jieba.cut('全由觀光局支付 相當可疑', cut_all=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=E, max_depth=md, min_samples_leaf=msl, max_features=mf, verbose=True, n_jobs=4)\n",
    "clf.fit(train_x, train_y)\n",
    "train_predicts = clf.predict(train_x)\n",
    "test_predicts = clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79       438\n",
      "           1       0.94      0.65      0.77       114\n",
      "           2       1.00      0.43      0.60        49\n",
      "           3       0.88      0.87      0.88      2739\n",
      "           4       1.00      0.10      0.18        10\n",
      "           5       1.00      0.17      0.29        41\n",
      "           6       0.98      0.67      0.79        78\n",
      "           7       1.00      0.12      0.22        74\n",
      "           8       0.94      0.63      0.76       771\n",
      "           9       0.97      0.42      0.59       285\n",
      "          10       0.88      0.90      0.89       674\n",
      "          11       0.63      0.89      0.74      1403\n",
      "          12       0.88      0.26      0.40       522\n",
      "          13       0.92      0.62      0.74       336\n",
      "          14       0.80      0.95      0.86      3322\n",
      "          15       0.81      0.82      0.82       969\n",
      "          16       0.94      0.61      0.74       101\n",
      "\n",
      "    accuracy                           0.81     11926\n",
      "   macro avg       0.91      0.58      0.65     11926\n",
      "weighted avg       0.83      0.81      0.80     11926\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.42      0.46       109\n",
      "           1       0.78      0.26      0.39        27\n",
      "           2       0.00      0.00      0.00        11\n",
      "           3       0.75      0.68      0.71       720\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       1.00      0.08      0.14        26\n",
      "           7       0.00      0.00      0.00        12\n",
      "           8       0.72      0.28      0.40       183\n",
      "           9       0.62      0.09      0.16        56\n",
      "          10       0.73      0.81      0.77       155\n",
      "          11       0.55      0.78      0.64       369\n",
      "          12       0.74      0.12      0.21       115\n",
      "          13       0.64      0.26      0.37        95\n",
      "          14       0.64      0.88      0.74       830\n",
      "          15       0.62      0.66      0.64       243\n",
      "          16       0.38      0.17      0.23        18\n",
      "\n",
      "    accuracy                           0.65      2982\n",
      "   macro avg       0.51      0.32      0.35      2982\n",
      "weighted avg       0.66      0.65      0.62      2982\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(train_y, train_predicts))\n",
    "print(classification_report(test_y, test_predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'自 中國 進口 口水 水壺 水壺蓋 壺蓋     太和 工房 被 爆 驗 出 殘渣 值 超標 57 倍'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(jieba.cut(df_data.iloc[1]['text'], cut_all=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8903236625859466"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3936955063715627"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  8,  3, ..., 11, 14,  3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
